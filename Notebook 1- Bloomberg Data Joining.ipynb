{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "93c14022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "top_50 = pd.ExcelFile('/Users/kirillsakharov/Desktop/bloomberg data/top_50.xlsx')\n",
    "top_50_remainder = pd.ExcelFile('/Users/kirillsakharov/Desktop/bloomberg data/top_50_remainder.xlsx')\n",
    "bottom_50 = pd.ExcelFile('/Users/kirillsakharov/Desktop/bloomberg data/50-100.xlsx')\n",
    "\n",
    "top_50_df = top_50.parse(sheet_name=0, skiprows=9, header=0)\n",
    "top_50_remainder_df = top_50_remainder.parse(sheet_name=0, skiprows=4, header=0)\n",
    "bottom_50_df = bottom_50.parse(sheet_name=0, skiprows=4, header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "16468b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b6e39e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_remainder_df = top_50_remainder_df.drop([\"Dates.1\", \"Dates.2\", \"Dates.3\", \"Dates.4\", \"Dates.5\", \"Unnamed: 20\",\n",
    "                                                \"Unnamed: 41\", \"Unnamed: 62\", \"Unnamed: 83\", \"Unnamed: 104\"], axis=1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "085d5980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning metric names followed by company name for each company.\n",
    "\n",
    "company_names = [\"UPS\", \"JPMorgan\", \"Kering\", \"Budweiser\", \"Toyota\", \"Toyota2\"]\n",
    "\n",
    "base_column_names = [\"Last Price\", \"Volume\", \"Volatility 10 Day\", \"Volatility 30 Day\", \n",
    "                \"Volatility 90 Day\", \"News Heat - Daily Average Story Flow\",\n",
    "                \"News Sentiment - Daily Average\", \"Twitter Sentiment Daily Average\",\n",
    "                \"Twitter Publication Count\", \"News Publication Count\", \n",
    "                \"ESG News Count ES Negative\", \"ESG News Count ES Positive\", \n",
    "                \"ESG News Sentiment ES Negative\", \"ESG News Sentiment ES Positive\", \n",
    "                \"Open Price\", \"Basic Earnings per Share\", \n",
    "                \"Price Earnings Ratio (P/E)\", \"Return on Common Equity\", \n",
    "                \"Total Debt to Total Equity\"]\n",
    "\n",
    "new_column_names = []\n",
    "\n",
    "for i, company in enumerate(company_names):\n",
    "    for base_name in base_column_names:\n",
    "        new_column = f\"{base_name}_{company}\"\n",
    "        new_column_names.append(new_column)\n",
    "\n",
    "top_50_remainder_df.columns = [\"Dates\"] + new_column_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f5098bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicate company\n",
    "cols_to_drop = [col for col in top_50_remainder_df.columns if col.endswith('Toyota2')]\n",
    "top_50_remainder_df = top_50_remainder_df.drop(cols_to_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "98bb13bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_df = top_50_df.drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9e47f5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning metric names followed by company name for each company.\n",
    "\n",
    "company_names = [\"Apple\", \"Google\", \"Google2\", \"Microsoft\", \"Amazon\", \"Meta\", \"Coke\", \"Disney\", \"Samsung\", \"Louis Vutton\", \"Louis Vutton2\", \"Mcdonalds\", \"Toyota\", \"Intel\", \"Nike\", \"AT&T\", \"Cisco\", \"Oracle\", \"Verizon\", \"Visa\", \"Walmart\", \"General Electric\", \"Budweiser\", \"Budweiser2\", \"SAP\", \"Mercedez\", \"IBM\", \"Philip Morris\", \"Netflix\", \"BMW\", \"American Express\", \"Honda\", \"Honda2\", \"L'oreal\", \"Gucci\", \"Hermes\", \"Nestle\", \"Home Depot\", \"Accenture\", \"Pepsi\", \"Starbucks\", \"Mastercard\", \"Frito(to delete)\", \"Zara\", \"Procter and Gamble/Gillete\", \"HSBC\", \"Volkswagen\", \"JPMorgan\", \"Sony\", \"Sony2\", \"UPS\", \"Bank of America\", \"JPM again!\"]\n",
    "\n",
    "base_column_names = [\"Last Price\", \"Volume\", \"Volatility 10 Day\", \"Volatility 30 Day\", \n",
    "                \"Volatility 90 Day\", \"News Heat - Daily Average Story Flow\",\n",
    "                \"News Sentiment - Daily Average\", \"Twitter Sentiment Daily Average\",\n",
    "                \"Twitter Publication Count\", \"News Publication Count\", \n",
    "                \"ESG News Count ES Negative\", \"ESG News Count ES Positive\", \n",
    "                \"ESG News Sentiment ES Negative\", \"ESG News Sentiment ES Positive\", \n",
    "                \"Open Price\", \"Basic Earnings per Share\", \n",
    "                \"Price Earnings Ratio (P/E)\", \"Return on Common Equity\", \n",
    "                \"Total Debt to Total Equity\"]\n",
    "\n",
    "new_column_names = []\n",
    "\n",
    "for i, company in enumerate(company_names):\n",
    "    for base_name in base_column_names:\n",
    "        new_column = f\"{base_name}_{company}\"\n",
    "        new_column_names.append(new_column)\n",
    "\n",
    "top_50_df.columns = [\"Dates\"] + new_column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f0c8eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing duplicates/ null valued stocks\n",
    "\n",
    "names_to_drop = ['Toyota2', 'Toyota', 'JPM again!', 'UPS', 'Sony2', 'JPMorgan', 'Frito(to delete)', 'Gucci', 'Honda2',\n",
    "                'Budweiser2', 'Budweiser', 'Louis Vutton2', 'Google2'] \n",
    "cols_to_drop = [col for col in top_50_df.columns if any(col.endswith(name) for name in names_to_drop)]\n",
    "top_50_df = top_50_df.drop(cols_to_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f5283861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning metric names followed by company name for each company.\n",
    "\n",
    "company_names = [\"Adidas\", \"Siemens\", \"CVS Health\", \"Porsche\", \"Citigroup\", \"Wells Fargo\", \"Adobe\", \"T-Mobile US\", \"eBay\", \"General Motors\", \"PayPal\", \"Ford\", \"HP Inc.\", \"Colgate-Palmolive\", \"Fox Corporation\", \"Lowe's\", \"H&M\", \"Lexus\", \"Banco Santander\", \"Costco\", \"Hyundai Motor Company\", \"Danone\", \"Heineken\", \"Goldman Sachs\", \"Nintendo\", \"Nintendo2\", \"AXA\", \"Allianz\", \"Dell\", \"Caterpillar\", \"John Deere\", \"UBS Group\", \"Yum! Brands\", \"Restaurant Brands International\", \"FedEx\", \"Volkswagen2\"]\n",
    "\n",
    "base_column_names = [\"Last Price\", \"Volume\", \"Volatility 10 Day\", \"Volatility 30 Day\", \n",
    "                \"Volatility 90 Day\", \"News Heat - Daily Average Story Flow\",\n",
    "                \"News Sentiment - Daily Average\", \"Twitter Sentiment Daily Average\",\n",
    "                \"Twitter Publication Count\", \"News Publication Count\", \n",
    "                \"ESG News Count ES Negative\", \"ESG News Count ES Positive\", \n",
    "                \"ESG News Sentiment ES Negative\", \"ESG News Sentiment ES Positive\", \n",
    "                \"Open Price\", \"Basic Earnings per Share\", \n",
    "                \"Price Earnings Ratio (P/E)\", \"Return on Common Equity\", \n",
    "                \"Total Debt to Total Equity\"]\n",
    "\n",
    "new_column_names = []\n",
    "\n",
    "for i, company in enumerate(company_names):\n",
    "    for base_name in base_column_names:\n",
    "        new_column = f\"{base_name}_{company}\"\n",
    "        new_column_names.append(new_column)\n",
    "\n",
    "bottom_50_df.columns = [\"Dates\"] + new_column_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "232dd949",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_50_df = bottom_50_df.iloc[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ac1a105b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_w/t13tb9qx6qsfqhrwbncrcc040000gn/T/ipykernel_2211/1940620531.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Dates'] = pd.to_datetime(df['Dates'])\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "dfs = [top_50_df, top_50_remainder_df, bottom_50_df]\n",
    "\n",
    "for df in dfs:\n",
    "    df['Dates'] = pd.to_datetime(df['Dates'])\n",
    "\n",
    "# Joining the dataframes on dates column\n",
    "merged_df = reduce(lambda left,right: pd.merge(left,right,on='Dates', how='outer'), dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "99144052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate column names found.\n"
     ]
    }
   ],
   "source": [
    "# Checking for any duplicates\n",
    "column_names = merged_df.columns.tolist()\n",
    "\n",
    "unique_column_names = set()\n",
    "for column_name in column_names:\n",
    "    if column_name in unique_column_names:\n",
    "        print(f\"Duplicate found: {column_name}\")\n",
    "    else:\n",
    "        unique_column_names.add(column_name)\n",
    "\n",
    "if len(column_names) == len(unique_column_names):\n",
    "    print(\"No duplicate column names found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785d3355",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# number of columns per stock\n",
    "n = 19  \n",
    "\n",
    "# gets the 'Last Price' column name for each stock\n",
    "stocks = [merged_df.columns[i] for i in range(1, len(merged_df.columns), n)]  \n",
    "\n",
    "for stock in stocks:\n",
    "    company_name = stock.split('_')[-1]  \n",
    "    for days in [1, 2, 3, 7, 15, 30]:\n",
    "        future_return = merged_df[stock].shift(-days) / merged_df[stock] - 1\n",
    "        future_return_col_name = f'{days} Day Future Return_{company_name}'\n",
    "        \n",
    "        # Find current column index and add future return column after\n",
    "        stock_idx = merged_df.columns.get_loc(stock)\n",
    "        merged_df.insert(stock_idx+1, future_return_col_name, future_return)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8b646133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CVS Health', 'HSBC', 'UBS Group', 'Ford', 'Dell', 'PayPal', 'Pepsi', 'Volkswagen2', 'Walmart', 'Oracle', 'AXA', 'Apple', 'Intel', 'Starbucks', 'John Deere', 'Netflix', 'Porsche', 'American Express', 'Nike', 'Mcdonalds', 'Danone', 'BMW', 'Mastercard', 'Louis Vutton', 'Costco', 'Hermes', 'UPS', 'Wells Fargo', 'Citigroup', 'Colgate-Palmolive', 'Meta', 'Mercedez', 'AT&T', 'Philip Morris', 'Bank of America', 'Siemens', 'Banco Santander', 'Adidas', 'H&M', 'Google', 'Nintendo', \"L'oreal\", 'Honda', 'Nintendo2', 'Caterpillar', 'Microsoft', 'Samsung', 'Cisco', 'Lexus', 'Fox Corporation', 'JPMorgan', 'Yum! Brands', 'SAP', 'Verizon', 'Amazon', 'Procter and Gamble/Gillete', 'Budweiser', 'Allianz', 'Accenture', 'General Electric', 'Disney', \"Lowe's\", 'Home Depot', 'Hyundai Motor Company', 'Nestle', 'HP Inc.', 'Adobe', 'Goldman Sachs', 'Dates', 'Coke', 'Sony', 'Zara', 'Heineken', 'FedEx', 'Toyota', 'General Motors', 'Kering', 'Volkswagen', 'eBay', 'Visa', 'IBM', 'T-Mobile US', 'Restaurant Brands International'}\n"
     ]
    }
   ],
   "source": [
    "# Extracting companies in dataframe\n",
    "companies = set()\n",
    "for col in merged_df.columns:\n",
    "    company_name = col.split('_')[-1]\n",
    "    companies.add(company_name)\n",
    "\n",
    "print(companies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "67f0a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate column names into two parts: feature and company\n",
    "merged_df.columns = pd.MultiIndex.from_tuples([col.split('_') if '_' in col else (col, 'None') for col in merged_df.columns])\n",
    "\n",
    "# Stack the DataFrame\n",
    "df_long = merged_df.stack().reset_index().rename(columns={'level_0': 'Date', 'level_1': 'Company'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "14a5ffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving for use in predictions in other notebooks\n",
    "df_long.to_csv('bloomberg_labelled.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
